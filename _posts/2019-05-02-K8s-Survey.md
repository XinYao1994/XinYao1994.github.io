---
layout:     post
title:      K8s Survey
subtitle:   New Blogs @ 2019/04/24
date:       2019/05/02
author:     Xin YAO
catalog: true
tags:
    - K8s
---

Installation
```
Normal Cluster and Deployment Launching - (private repo:xyao/k8s)
Using RDMA - (https://github.com/hustcat/k8s-rdma-device-plugin)
- 
- Environment="KUBELET_EXTRA_ARGS=--feature-gates=DevicePlugins=true XXX"
```

Resource Allocation

Create a LimitRange, which specifies a default memory request and a default memory limit.
```
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
```
```
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-min-max-demo-lr
spec:
  limits:
  - max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
```
The Containerâ€™s memory request is set to match its memory limit, but the request will not affect its memory limit.
```
containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
```
Specify a memory request that is too big for your Nodes, the Pod status is PENDING. That is, the Pod is not scheduled to run on any Node.

If you do not specify a memory limit for a Container, one of the following situations applies:

1. The Container has no upper bound on the amount of memory it uses. The Container could use all of the memory available on the Node where it is running which in turn could invoke the OOM Killer. Further, in case of an OOM Kill, a container with no resource limits will have a greater chance of being killed.
2. The Container is running in a namespace that has a default memory limit, and the Container is automatically assigned the default limit. Cluster administrators can use a LimitRange to specify a default value for the memory limit.

Similar to Memory
```
apiVersion: v1
kind: Pod
metadata:
  name: default-cpu-demo
spec:
  containers:
  - name: default-cpu-demo-ctr
    image: nginx
```
```
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
spec:
  limits:
  - max:
      cpu: "800m"
    min:
      cpu: "200m"
    type: Container
```
```
containers:
  - name: cpu-demo-ctr
    image: vish/stress
    resources:
      limits:
        cpu: "1"
      requests:
        cpu: "0.5"
```
```
resources:
  limits:
    cpu: "1"
  requests:
    cpu: 500m
```
```
resources:
  limits:
    cpu: "100"
  requests:
    cpu: "100"
```
If you do not specify a CPU limit for a Container, then one of these situations applies:

1. The Container has no upper bound on the CPU resources it can use. The Container could use all of the CPU resources available on the Node where it is running.
2. The Container is running in a namespace that has a default CPU limit, and the Container is automatically assigned the default limit. Cluster administrators can use a LimitRange to specify a default value for the CPU limit.

Create a ResourceQuota
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
```
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pod-demo
spec:
  hard:
    pods: "2"
```
The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:
1. Every Container must have a memory request, memory limit, cpu request, and cpu limit.
2. The memory request total for all Containers must not exceed 1 GiB.
3. The memory limit total for all Containers must not exceed 2 GiB.
4. The CPU request total for all Containers must not exceed 1 cpu.
5. The CPU limit total for all Containers must not exceed 2 cpu

A pods with mutiple containers:
```
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-4
  namespace: qos-example
spec:
  containers:

  - name: qos-demo-4-ctr-1
    image: nginx
    resources:
      requests:
        memory: "200Mi"

  - name: qos-demo-4-ctr-2
    image: redis
```

Volume:
```
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
```
Create a PeresistentVolume
```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```
Create a PersistentVolumeClaim
```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```
Mount a PersistentVolume
```
kind: Pod
apiVersion: v1
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
       claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

Start from a private registry - (private repo:xyao/k8s/repo)

Assign Pods to Nodes
1. use label and nodeSelector
2. use Daemonset

Create a Pod that has an Init Container
```
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: workdir
      mountPath: /usr/share/nginx/html
  # These containers are run during pod initialization
  initContainers:
  - name: install
    image: busybox
    command:
    - wget
    - "-O"
    - "/work-dir/index.html"
    - http://kubernetes.io
    volumeMounts:
    - name: workdir
      mountPath: "/work-dir"
  dnsPolicy: Default
  volumes:
  - name: workdir
    emptyDir: {}
```

postStart and preStop handlers
```
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
      preStop:
        exec:
          command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
```

ConfigMap - https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/

Accessing services
1. Ingress
2. services and kubectl expose 

Weave Network - https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#npc

GPU - https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/



